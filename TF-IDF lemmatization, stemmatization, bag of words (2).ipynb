{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a2654a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words tokenization\n",
      "\n",
      "['this', 'is', 'a', 'sample', 'sequence', '.', 'tokenize', 'me', ',', 'please']\n",
      "\n",
      "sentence tokenize\n",
      "\n",
      "['this is a sample sequence.', 'tokenize me, please']\n"
     ]
    }
   ],
   "source": [
    "#bagofwords\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "text=\"this is a sample sequence. tokenize me, please\"\n",
    "\n",
    "#tokenize text into words\n",
    "words=word_tokenize(text)\n",
    "print(\"words tokenization\\n\")\n",
    "print(words)\n",
    "\n",
    "#tokenize text into sentences\n",
    "sentence=sent_tokenize(text)\n",
    "print(\"\\nsentence tokenize\\n\")\n",
    "print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b0ef6c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal Words\n",
      " ['Jumping', 'Jumps', 'Jumped', 'Running', 'Runs', 'Runner', 'Easily']\n",
      "Stemmanized Words  ['jump', 'jump', 'jump', 'run', 'run', 'runner', 'easili']\n"
     ]
    }
   ],
   "source": [
    "#stemmatization\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer=PorterStemmer()\n",
    "\n",
    "words=['Jumping','Jumps','Jumped','Running','Runs','Runner','Easily']\n",
    "stemmed_words=[stemmer.stem(words) for words in words]\n",
    "\n",
    "print(\"Orginal Words\\n\",words)\n",
    "print(\"Stemmanized Words \",stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "828e2e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal Words\n",
      " ['Jumping', 'Jumps', 'Jumped', 'Running', 'Caring', 'Runner', 'Easily']\n",
      "Stemmanized Words  ['Jumping', 'Jumps', 'Jumped', 'Running', 'Caring', 'Runner', 'Easily']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mujta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#lemmatization\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "words=['Jumping','Jumps','Jumped','Running','Caring','Runner','Easily']\n",
    "lemmatized_words=[lemmatizer.lemmatize(words) for words in words]\n",
    "\n",
    "print(\"Orginal Words\\n\",words)\n",
    "print(\"Stemmanized Words \",lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1f023bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal\n",
      " this is a sample sentence with some stopwords that we want to remove\n",
      "Stopwords implementation\n",
      " ['sample', 'sentence', 'stopwords', 'want', 'remove']\n"
     ]
    }
   ],
   "source": [
    "#stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text=\"this is a sample sentence with some stopwords that we want to remove\"\n",
    "words=word_tokenize(text)\n",
    "\n",
    "stop_words=set(stopwords.words('english'))\n",
    "filtered_words= [words for words in words if words.lower() not in stop_words]\n",
    "print(\"Orginal\\n\",text)\n",
    "print(\"Stopwords implementation\\n\",filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7b7708b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['environment conserv crucial rapidli chang world .', 'it safeguard ecosystem , preserv biodivers , ensur sustain futur .', 'by reduc pollut , conserv resourc , protect natur habitat , mitig devast effect climat chang maintain harmoni balanc human planet .', 'our collect respons embrac conserv effort safeguard preciou earth .']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "para = \"Environmental conservation is crucial in our rapidly changing world. It safeguards ecosystems, preserves biodiversity, and ensures a sustainable future. By reducing pollution, conserving resources, and protecting natural habitats, we can mitigate the devastating effects of climate change and maintain a harmonious balance between humanity and the planet. Our collective responsibility is to embrace conservation efforts and safeguard our precious Earth.\"\n",
    "stemmer = PorterStemmer()\n",
    "sentence = sent_tokenize(para)\n",
    "\n",
    "# For stemming\n",
    "for i in range(len(sentence)):\n",
    "    words = word_tokenize(sentence[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentence[i] = ' '.join(words)\n",
    "\n",
    "print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e84f0eac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Environmental conservation crucial rapidly changing world .', 'It safeguard ecosystem , preserve biodiversity , ensures sustainable future .', 'By reducing pollution , conserving resource , protecting natural habitat , mitigate devastating effect climate change maintain harmonious balance humanity planet .', 'Our collective responsibility embrace conservation effort safeguard precious Earth .']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "para = \"Environmental conservation is crucial in our rapidly changing world. It safeguards ecosystems, preserves biodiversity, and ensures a sustainable future. By reducing pollution, conserving resources, and protecting natural habitats, we can mitigate the devastating effects of climate change and maintain a harmonious balance between humanity and the planet. Our collective responsibility is to embrace conservation efforts and safeguard our precious Earth.\"\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sentence = sent_tokenize(para)\n",
    "\n",
    "for i in range(len(sentence)):\n",
    "    words = word_tokenize(sentence[i])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentence[i] = ' '.join(words)\n",
    "\n",
    "print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd3363e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF for Document 1:{'the': 0.0, 'cat': 0.029348543175946873, 'chased': 0.07952020911994373, 'mouse': 0.029348543175946873, '.': 0.0}\n",
      "TF-IDF for Document 2:{'the': 0.0, 'cat': 0.03521825181113625, 'was': 0.09542425094393249, 'fast': 0.09542425094393249, '.': 0.0}\n",
      "TF-IDF for Document 3:{'the': 0.0, 'mouse': 0.03521825181113625, 'ran': 0.09542425094393249, 'away': 0.09542425094393249, '.': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"The cat chased the mouse.\",\n",
    "    \"The cat was fast.\",\n",
    "    \"The mouse ran away.\"\n",
    "]\n",
    "\n",
    "#Tokenizing\n",
    "words = [word_tokenize(doc.lower()) for doc in documents]\n",
    "\n",
    "#unique words\n",
    "unique_words=set([word for doc_words in words for word in doc_words])\n",
    "\n",
    "#TF term frequence calculation\n",
    "tf_values=[{word: freq/len(words[i]) for word, freq in FreqDist(doc).items()} for i,doc in enumerate(words)]\n",
    "\n",
    "#DF document frequency\n",
    "df_values={word: sum(word in doc for doc in words) for word in unique_words}\n",
    "\n",
    "#inverse document frequency idf\n",
    "idf_values={word: math.log10(len(documents)/df) for word, df in df_values.items()}\n",
    "\n",
    "#TF-IDF\n",
    "tfidf_values=[{word: tf*idf_values[word] for word, tf in tf.items()} for tf in tf_values]\n",
    "\n",
    "for i, doc in enumerate(tfidf_values):\n",
    "    print(f\"TF-IDF for Document {i+1}:{doc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b166be91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t0.40352535506797127\n",
      "  (0, 2)\t0.5305873490316616\n",
      "  (0, 1)\t0.40352535506797127\n",
      "  (0, 6)\t0.6267468712982053\n",
      "  (1, 3)\t0.5844829010200651\n",
      "  (1, 7)\t0.5844829010200651\n",
      "  (1, 1)\t0.444514311537431\n",
      "  (1, 6)\t0.34520501686496574\n",
      "  (2, 0)\t0.5844829010200651\n",
      "  (2, 5)\t0.5844829010200651\n",
      "  (2, 4)\t0.444514311537431\n",
      "  (2, 6)\t0.34520501686496574\n",
      "Documents 1 : The cat chased the mouse.\n",
      " away: 0.0\n",
      " cat: 0.40352535506797127\n",
      " chased: 0.5305873490316616\n",
      " fast: 0.0\n",
      " mouse: 0.40352535506797127\n",
      " ran: 0.0\n",
      " the: 0.6267468712982053\n",
      " was: 0.0\n",
      "\n",
      "Documents 2 : The cat was fast.\n",
      " away: 0.0\n",
      " cat: 0.444514311537431\n",
      " chased: 0.0\n",
      " fast: 0.5844829010200651\n",
      " mouse: 0.0\n",
      " ran: 0.0\n",
      " the: 0.34520501686496574\n",
      " was: 0.5844829010200651\n",
      "\n",
      "Documents 3 : The mouse ran away.\n",
      " away: 0.5844829010200651\n",
      " cat: 0.0\n",
      " chased: 0.0\n",
      " fast: 0.0\n",
      " mouse: 0.444514311537431\n",
      " ran: 0.5844829010200651\n",
      " the: 0.34520501686496574\n",
      " was: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sk Learn TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = [\n",
    "    \"The cat chased the mouse.\",\n",
    "    \"The cat was fast.\",\n",
    "    \"The mouse ran away.\"\n",
    "]\n",
    "#intialize TFIDVECTORIZER\n",
    "tfidf=TfidfVectorizer()\n",
    "\n",
    "#compute TF-IDF\n",
    "tfidf_matrix=tfidf.fit_transform(documents)\n",
    "\n",
    "#display TF-IDF matrix\n",
    "print(tfidf_matrix)\n",
    "\n",
    "#features name\n",
    "features_names=tfidf.get_feature_names_out()\n",
    "\n",
    "#display TF-IDF result\n",
    "for i, document in enumerate(documents):\n",
    "    print(f\"Documents {i+1} : {document}\")\n",
    "    for idx, term in enumerate(features_names):\n",
    "        print(f\" {term}: {tfidf_matrix[i,idx]}\")\n",
    "    print()\n",
    "              \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b66fcc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
